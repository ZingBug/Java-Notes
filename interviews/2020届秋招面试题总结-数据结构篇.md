# 2020届秋招面试题总结——数据结构篇

**1、ConcurrentHashMap为何读不加锁。**

在JDK 1.8中，ConcurrentHashMap采用Node+CAS+Synchronized来保证并发安全，在读操作时没有加锁操作。先看一下ConcurrentHashMap读操作的相关源码。

```java
transient volatile Node<K,V>[] table;//代表table的地址是volatile，而不是数组元素是volatile。这处是为了使得table在扩容时对其他线程也保持可见性。
static class Node<K,V> implements Map.Entry<K,V> {
    final int hash;
    final K key;
    //用volatile修饰Node的元素val和指针next，在多线程环境下保持可见性。
    volatile V val;
    volatile Node<K,V> next;
    Node(int hash, K key, V val, Node<K,V> next) {
        this.hash = hash;
        this.key = key;
        this.val = val;
        this.next = next;
    }
    //省略其他代码
}
//在读操作中没有一处加锁。
public V get(Object key) {
    Node<K,V>[] tab; Node<K,V> e, p; int n, eh; K ek;
    int h = spread(key.hashCode());//计算hash值
    if ((tab = table) != null && (n = tab.length) > 0 &&
        (e = tabAt(tab, (n - 1) & h)) != null) {//读取首节点的Node元素
        if ((eh = e.hash) == h) {//如果Key相等则直接返回首节点
            if ((ek = e.key) == key || (ek != null && key.equals(ek)))
                return e.val;
        }
        // eh（hash值）为负值表示正在扩容，这个时候查的是ForwardingNode的find方法来定位到nextTable。
        // eh=-1，说明该节点是一个ForwardingNode，正在迁移，此时调用ForwardingNode的find方法去nextTable里找。
        // eh=-2，说明该节点是一个TreeBin，此时调用TreeBin的find方法遍历红黑树，由于红黑树有可能正在旋转变色，所以find里会有读写锁。
        // eh>=0，说明该节点下挂的是一个链表，直接遍历该链表即可。

        else if (eh < 0)
            return (p = e.find(h, key)) != null ? p.val : null;
        while ((e = e.next) != null) {//直接在链表遍历查询
            if (e.hash == h &&
                ((ek = e.key) == key || (ek != null && key.equals(ek))))
                return e.val;
        }
    }
    return null;
}
```

总结：

- 在JDK 1.8中，ConcurrentHashMap的get操作不需要加锁，这也是它比其他并发集合比如HashTable、用Collections.synchronizedMap()包装的HashMap更为安全高效的原因。
- get操作全程不需要加锁是因为Node的成员val是用volatile修饰的，**和数组用volatile修饰没有关系**。
- 数组用volatile修饰主要是保证数组在扩容的时候保持可见性。

**2、HashMap如何同步。**

HashMap有很多优点，但也有一个缺点：不是同步的。

当多线程并发访问一个哈希表时，需要在外部进行同步操作，否则会引发数据不同步问题。

当我们需要一个同步的HashMap时，有两种选择：

- 选择全局加锁，考虑用Collections.synchronizedMap包一层锁，变成个线程安全的Map：Map m = Collections.synchronizedMap(new HashMap(...));
- 使用ConcurrentHashMap。这两个选项之间的首选是使用ConcurrentHashMap，这是因为我们不需要锁定整个对象。

此外，刚才提到Collections.synchronizedMap，这里单独多说一点，它的原理超级简单，就是内部多加一个锁（在构造函数中初始化为SynchronizedMap类本身），每次调用get、put等方法时都用synchronized方法块来实现同步功能。

```java
private static class SynchronizedMap<K,V>
    implements Map<K,V>, Serializable {
    private static final long serialVersionUID = 1978198479659022715L;
    private final Map<K,V> m;     // Backing Map
    final Object      mutex;        // Object on which to synchronize
    SynchronizedMap(Map<K,V> m) {
        this.m = Objects.requireNonNull(m);
        mutex = this;
    }
    SynchronizedMap(Map<K,V> m, Object mutex) {
        this.m = m;
        this.mutex = mutex;
    }
    public int size() {
        synchronized (mutex) {return m.size();}
    }
    public V get(Object key) {
        synchronized (mutex) {return m.get(key);}
    }
    public V put(K key, V value) {
        synchronized (mutex) {return m.put(key, value);}
    }
    public V remove(Object key) {
        synchronized (mutex) {return m.remove(key);}
    }
        // 省略其他方法
}
```

**3、说说常见的集合有哪些吧。**

Map接口和Collection接口是所有集合类框架的父接口：

Collcetion接口的子接口包括：Set接口和List接口。

- Map接口的实现类主要有：HashMap、HashTable、ConcurrentHashMap以及Properties等。
- Set接口的实现类主要有：HashSet、TreeSet、LinkedHashSet等。
- List接口的实现类主要有：ArrayList、LinkedList、Stack以及Vector等。

如此图所示（网上找的，有点丑，凑付看吧）。

![Y5FXy6.jpg](https://s1.ax1x.com/2020/05/19/Y5FXy6.jpg)

**4、HashMap与HashTable的区别。**

主要有以下几点区别。

- HashMap没有考虑同步，是线程不安全的；HashTable在关键方法（put、get、contains、size等）上使用了Synchronized关键字，是线程安全的。
- HashMap允许Key/Value都为null，后者Key/Value都不允许为null。
- HashMap继承自AbstractMap类；而HashTable继承自Dictionary类（较为陈旧）。
- 在jdk1.8中，HashMap的底层结构是数组+链表+红黑树，而HashTable的底层结构是数组+链表。
- HashMap对底层数组采取的懒加载，即当执行第一次put操作时才会创建数组；而HashTable在初始化时就创建了数组。
- HashMap中数组的默认初始容量是16，并且必须是2的指数倍数，扩容时`newCapacity=2*oldCapacity；`而HashTable中默认的初始容量是11，并且不要求必须是2的指数倍数，扩容时`newCapacity=2*oldCapacity+1`。
- 在hash取模计算时，HashTable的模数一般为素数，简单的做除取模结果会更为均匀，`int index = (hash & 0x7FFFFFFF) % tab.length;`
- HashMap的模数为2的幂，直接用位运算来得到结果，效率要大大高于做除法，`i = (n - 1) & hash；`当然，为了解决哈希分布不均匀情况，所以在hash算法中引入了扰动计算，这又是后话了。

**5、HashMap的put方法的具体流程。**

在jdk1.8以前的HashMap的实现是数组+链表，即使哈希函数取得再好，也很难达到元素百分百均匀分布。当HashMap中有大量的元素都存放在同一个桶中时，这个桶下有一条长长的链表，这个时候HashMap就相当于一个单链表，假如单链表有n个元素，遍历的时间复杂度就是O(n)，完全失去了它的优势。

针对这种情况，JDK1.8引入了红黑树（查找时间复杂度为O(logn)）来优化这个问题。

下面具体看put方法的源码。

```java
public V put(K key, V value) {
    return putVal(hash(key), key, value, false, true);
}

final V putVal(int hash, K key, V value, boolean onlyIfAbsent,
               boolean evict) {
    Node<K,V>[] tab; Node<K,V> p; int n, i;
    // 1 如果table为空或者长度为0，即没有元素，则使用resize()方法扩容
    if ((tab = table) == null || (n = tab.length) == 0)
        n = (tab = resize()).length;
    // 2 计算插入存储的数组索引i，如果数组当前位置为空，则不存在Hash冲突，可以直接插入元素。
    if ((p = tab[i = (n - 1) & hash]) == null)
        tab[i] = newNode(hash, key, value, null);
    // 3 插入新元素时，如果发生Hash冲突，则依次往下判断。
    else {
        Node<K,V> e; K k;
        // 3.1 判断table[i]的元素的key是否与需要插入的key相同，若相同则直接用新的value覆盖掉旧的value，判断元素相等原则是equals()，所以key对象需要重写该方法。
        if (p.hash == hash &&
            ((k = p.key) == key || (key != null && key.equals(k))))
            e = p;
        // 3.2 判断需要插入的数据结构是红黑树还是链表，如果是红黑树，则直接在树中通过putTreeVal()方法插入/更新键值对。
        else if (p instanceof TreeNode)
            e = ((TreeNode<K,V>)p).putTreeVal(this, tab, hash, key, value);
        // 3.3 如果是链表，则在链表中插入/更新键值对
        else {
            // 3.3.1 遍历table[i]，判断key是否已经存在，采用equals对比当前遍历结点的key与需要插入数据的key，如果存在相同，则直接覆盖。
            // 3.3.2 遍历完毕后如果没有发现相同的key，直接在链表尾部插入新的节点元素。
            // 插入完成后判断链表长度是否>TREEIFY_THRESHOLD（8），若是，则把链表转换为红黑树。
            for (int binCount = 0; ; ++binCount) {
                if ((e = p.next) == null) {
                    p.next = newNode(hash, key, value, null);
                    if (binCount >= TREEIFY_THRESHOLD - 1) // -1 for 1st
                        treeifyBin(tab, hash);
                    break;
                }
                if (e.hash == hash &&
                    ((k = e.key) == key || (key != null && key.equals(k))))
                    break;
                p = e;
            }
        }
        // 3.4  如果e不为空，证明key已经存在，直接用新的value覆盖旧的value即可。
        if (e != null) { // existing mapping for key
            V oldValue = e.value;
            if (!onlyIfAbsent || oldValue == null)
                e.value = value;
            afterNodeAccess(e);
            return oldValue;
        }
    }
    ++modCount;
    // 4 插入成功后，判断实际存在的键值对数量size>最大容量，如果大于则进行扩容。
    if (++size > threshold)
        resize();
    // 5 插入成功时会调用的方法（默认实现为空）
    afterNodeInsertion(evict);
    return null;
}
```

简单的图片总结为：

![Y5k8XV.png](https://s1.ax1x.com/2020/05/19/Y5k8XV.png)

HashMap中的红黑树操作具体可以参考：[Java 集合深入理解（17）：HashMap 在 JDK 1.8 后新增的红黑树结构](https://blog.csdn.net/u011240877/article/details/53358305)

**6、HashMap的扩容操作是怎么实现的。**

在HashMap中，通过resize()方法来进行扩容或者初始化操作。直接上源码。

```java
final Node<K,V>[] resize() {
        // 1 复制一份扩容前的数组（当前数组）
    Node<K,V>[] oldTab = table;
        // 2 保存旧的数组长度，阈值
    int oldCap = (oldTab == null) ? 0 : oldTab.length;
    int oldThr = threshold;
    int newCap, newThr = 0;
        // 3 判断扩容前的数组容量
    if (oldCap > 0) {
        // 3.1 若扩容前的数组容量超过最大值，则不再扩容。
        if (oldCap >= MAXIMUM_CAPACITY) {
            threshold = Integer.MAX_VALUE;
            return oldTab;
        }
        // 3.2 若没有超过最大值，则扩容为原来二倍。
        else if ((newCap = oldCap << 1) < MAXIMUM_CAPACITY &&
                 oldCap >= DEFAULT_INITIAL_CAPACITY)
            newThr = oldThr << 1; // double threshold
    }
    // 4 如果旧容量为0，并且旧阈值>0，说明之前创建了哈希表但没有添加元素，初始化容量等于阈值。
    else if (oldThr > 0) // initial capacity was placed in threshold
        newCap = oldThr;
    // 5 如果旧容量、旧阈值都为0，说明还没创建哈希表，容量为默认值，阈值为容量*加载因子。
    else {               // zero initial threshold signifies using defaults
        newCap = DEFAULT_INITIAL_CAPACITY;
        newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY);
    }
        // 6 如果新的阈值为0，则用新容量*加载因子 重新计算一次。
    if (newThr == 0) {
        float ft = (float)newCap * loadFactor;
        newThr = (newCap < MAXIMUM_CAPACITY && ft < (float)MAXIMUM_CAPACITY ?
                  (int)ft : Integer.MAX_VALUE);
    }
    // 7 更新阈值。
    threshold = newThr;
    // 8 创建新链表数组，容量是原来两倍。
    @SuppressWarnings({"rawtypes","unchecked"})
        Node<K,V>[] newTab = (Node<K,V>[])new Node[newCap];
    table = newTab;
    // 9 接下来是遍历复制。
    if (oldTab != null) {
        for (int j = 0; j < oldCap; ++j) {
            Node<K,V> e;
            if ((e = oldTab[j]) != null) {
                // 9.1 旧的桶置为空
                oldTab[j] = null;
                // 9.2 如果当前桶只有一个元素，则直接赋值给新table的对应位置。
                if (e.next == null)
                    newTab[e.hash & (newCap - 1)] = e;
                // 9.3 如果当前桶是树形结构，则要把新table当前位置桶也变成树结构。
                else if (e instanceof TreeNode)
                    ((TreeNode<K,V>)e).split(this, newTab, j, oldCap);
                // 9.4 保留旧哈希表桶中链表元素的顺序
                else { // preserve order
                    Node<K,V> loHead = null, loTail = null;//按原始链表顺序，过滤出来扩容后位置不变的元素（低位=0），放在一起。
                    Node<K,V> hiHead = null, hiTail = null;//按原始链表顺序，过滤出来扩容后位置改变到（index+oldCap）的元素（高位=0），放在一起。
                    Node<K,V> next;
                    // 9.4.1 do-while循环赋值给新哈希表
                    do {
                        next = e.next;
                        // 9.4.2 通过计算(e.hash & oldCap) == 0分成两条链表，再将两条链表散列到新数组的不同位置。
                        if ((e.hash & oldCap) == 0) {
                            if (loTail == null)
                                loHead = e;
                            else
                                loTail.next = e;
                            loTail = e;
                        }
                        else {
                            if (hiTail == null)
                                hiHead = e;
                            else
                                hiTail.next = e;
                            hiTail = e;
                        }
                    } while ((e = next) != null);
                    // 9.4.3 放到原索引位置不变的桶中。
                    if (loTail != null) {
                        loTail.next = null;
                        newTab[j] = loHead;
                    }
                    // 9.4.4 放到原索引+oldCap位置的桶中。
                    if (hiTail != null) {
                        hiTail.next = null;
                        newTab[j + oldCap] = hiHead;
                    }
                }
            }
        }
    }
    return newTab;
}
```

有一点注意区别，JDK1.7中resize的时候，旧链表迁移新链表的时候，如果在新表的数组索引位置相同，则链表元素会倒置，但在JDK1.8不会倒置，链表元素还是按照之前顺序。

**7、HashMap是怎么解决哈希冲突的。**

哈希冲突：当两个不同的输入值，根据同一散列函数计算出相同的散列值的现象，我们就把它叫做哈希碰撞。

在Java中，保存数据有两种比较简单的数据结构：数组和链表。数组的特点是：寻址容易，插入和删除困难。链表的特点是：寻址困难，但插入和删除容易。所以我们将数组和链表结合在一起，发挥两者的优势，使用一种叫做链地址法的方式来解决哈希冲突。

![Y5kv3n.png](https://s1.ax1x.com/2020/05/19/Y5kv3n.png)

这样我们就可以拥有相同哈希值的对象组织成的一个链表放在hash值对应的bucket下，但相比Key.hashCode()返回的int类型，我们HashMap初始的**容量大小DEFAULT_INITIAL_CAPACITY = 1 << 4（即2的四次方为16）要远小于int类型的范围**，所以我们如果只是单纯的使用hashcode取余来获取对应位置的bucket，这将会大大增加哈希碰撞的几率，并且最坏情况下还会将HashMap变成一个单链表。所以肯定要对hashCode做一定优化。

来看HashMap的hash()函数。上面提到的问题，主要是因为如果使用hashCode取余，那么相当于参与运算的只有hashCode的低位，高位是没有起到任何作用的，所以我们的思路就是让**hashCode取值出的高位也参与运算，进一步降低hash碰撞的概率，使得数据分布更平均，我们把这样的操作称为扰动，**在JDK 1.8中的hash()函数如下：

```java
static final int hash(Object key) {
    int h;
    return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
}
```

相比在JDK 1.7中的4次位运算，5次异或运算（9次扰动），在1.8中，只进行了1次位运算和1次异或运算（2次扰动），更为简洁了。两次扰动已经达到了高低位同时参与运算的目的，提高了对应数组存储下标位置的随机性和均匀性。

通过上面的链地址法（使用散列表）和扰动函数，数据分布更为均匀，哈希碰撞也减少了。但是当HashMap中存在大量的数据时，假如某个bucket下对应的链表中有n个元素，那么遍历时间复杂度就变成了O(n)，针对这个问题，JDK 1.8在HashMap中新增了红黑树的数据结构，进一步使得遍历复杂度降低至O(logn)。

简单**总结**一下HashMap是如何有效解决哈希碰撞的：

- 使用链地址法（散列表）来链接拥有相同hash值的元素；
- 使用2次扰动（hash函数）来降低哈希冲突的概率，使得概率分布更为均匀；
- 引入红黑树进一步降低遍历的时间复杂度。

参考链接：[全网把Map中的hash()分析的最透彻的文章，别无二家。](https://juejin.im/post/5ab99afff265da23a2291dee)

**8、HashMap为什么不直接使用hashCode()处理后的哈希值之直接作为table的下标。**

hashCode()方法返回的是int整数类型，其范围为-(2 ^ 31)~(2 ^ 31 - 1)，约有40亿个映射空间，而HashMap的容量范围是在16（初始化默认值）~2 ^ 30，HashMap通常情况下是取不到最大值的，并且设备上也难以提供这么多的存储空间，从而导致通过hashCode()计算出的哈希值可能不在数组大小范围内，进而无法匹配存储位置。

所以解决方法为：

- 在HashMap中实现了自己的hash()方法，通过两次扰动使得元素对象的哈希值高低位自行进行异或运算，降低哈希碰撞概率也使得数据分布更平均。（具体看上一题）
- 在保证数组长度为2的幂次方的时候，使用hash()运算之后的值与运算（&）（数组长度 - 1）来获取数组下标的方式进行存储，这样一来是比取余操作更加有效率，二来也是因为只有当数组长度为2的幂次方时，h&(length-1)才等价于h%length，三来解决了“哈希值与数组大小范围不匹配”的问题。

为什么数组长度要保证2的幂次方：

- 只有当数组长度为2的幂次方时，h&(length-1)才等价于h%length，可以用位运算来代替做除取模运算，实现了key的定位，2的幂次方也可以减少冲突次数，提高HashMap的查询效率；
- 如果 length 为 2 的次幂 则 length-1 转化为二进制必定是 11111……的形式，在于 h 的二进制与操作效率会非常的快，而且空间不浪费；如果 length 不是 2 的次幂，比如 length 为 15，则 length - 1 为 14，对应的二进制为 1110，在于 h 与操作，最后一位都为 0 ，而 0001，0011，0101，1001，1011，0111，1101 这几个位置永远都不能存放元素了，空间浪费相当大，更糟的是这种情况中，数组可以使用的位置比数组长度小了很多，这意味着进一步增加了碰撞的几率，减慢了查询的效率！这样就会造成空间的浪费。
- 当然，HashTable就没有采用2的幂作为数组长度，而是采用素数。素数的话是用简单做除取模方法来获取下标index，而不是位运算，效率低了不少，但分布也很均匀。

**9、为什么HashMap中String、Integer这样的包装类适合作为Key。**

String、Integer等包装类的特性能够保证Hash值的不可更改性和计算准确性，能够有效的减少Hash碰撞的几率。

- 都是final类型，即不可变性，保证key的不可更改性，不会存在获取hash值不同的情况。
- 内部已经重写equals()、hashCode()等方法，遵守了HashMap内部的规范，不容易出现Hash值计算错误的情况。

如果想要自己写的类作为Key，需要重写hashCode()和equals()方法。

- 重写HashCode()是因为需要计算存储数据的存储位置，需要注意，不要试图从散列码计算中排除掉一个对象的关键部分来提高性能，这样虽然能更快但是可能会导致更多的Hash碰撞。
- 重写equals()方法，需要遵守自反性、对称性、传递性、一致性以及对于任务非null的引用之x，x.equals(null)必须返回false这几个特性，目的就是为了保证Key在哈希表中的唯一性。

**10、ConcurrentHashMap和HashMap的区别。**

ConcurrentHashMap结合了HashMap和HashTable二者的优势。HashMap没有考虑同步，HashTable却要在每次同此执行时都要锁住整个结构（用Synchronized关键字）。ConcurrentHashMap锁的方式是稍微细粒度的。

在JDK 1.8中，ConcurrentHashMap放弃了臃肿的分段锁设计，取而代之的是采用Node+CAS+Synchronized来保证并发安全进行实现。

以插入元素（put方法）为例。

```java
public V put(K key, V value) {
    return putVal(key, value, false);
}
final V putVal(K key, V value, boolean onlyIfAbsent) {
    // 1 key和value不能为null。
    if (key == null || value == null) throw new NullPointerException();
    // 2 通过spread函数获取hash值（与HashMap类似，也加入了扰动）
    int hash = spread(key.hashCode());
    int binCount = 0;//用来计算在这个桶总共有多少元素，用来控制扩容或者转换为红黑树。
    for (Node<K,V>[] tab = table;;) {
        Node<K,V> f; int n, i, fh;
        // 3 table为空，则先初始化数组。
        if (tab == null || (n = tab.length) == 0)
            tab = initTable();
        // 4 如果table对应位置为空，则调用CAS插入相应的数据，注意这个地方没有加锁。
        else if ((f = tabAt(tab, i = (n - 1) & hash)) == null) {
            if (casTabAt(tab, i, null,
                         new Node<K,V>(hash, key, value, null)))
                break;                   // no lock when adding to empty bin
        }
        // 5 如果取出来的节点的hash值是MOVED(-1)的话，则表示当前正在对这个数组进行扩容，复制到新的数组，则当前线程也去帮助复制。
        else if ((fh = f.hash) == MOVED)
            tab = helpTransfer(tab, f);
        else {
            V oldVal = null;
            // 6 如果这个位置有元素的话，则对该节点加synchronized锁。
            synchronized (f) {
                if (tabAt(tab, i) == f) {
                    // 7 如果是链表的话(hash大于0)，就对这个链表的所有元素进行遍历。
                    if (fh >= 0) {
                        binCount = 1;
                        for (Node<K,V> e = f;; ++binCount) {
                            K ek;
                            // 8 若找到Key和Key的hash值都一样的节点，则用新的value值替换旧值。
                            if (e.hash == hash &&
                                ((ek = e.key) == key ||
                                 (ek != null && key.equals(ek)))) {
                                oldVal = e.val;
                                if (!onlyIfAbsent)
                                    e.val = value;
                                break;
                            }
                            Node<K,V> pred = e;
                            // 9 若没有找到相同Key节点的话，则在链表后面添加新的节点。
                            if ((e = e.next) == null) {
                                pred.next = new Node<K,V>(hash, key,
                                                          value, null);
                                break;
                            }
                        }
                    }
                    // 10 当前桶的数据结构为红黑树时，调用putTreeVal方法将元素添加到树中。
                    else if (f instanceof TreeBin) {
                        Node<K,V> p;
                        binCount = 2;
                        if ((p = ((TreeBin<K,V>)f).putTreeVal(hash, key,
                                                       value)) != null) {
                            oldVal = p.val;
                            if (!onlyIfAbsent)
                                p.val = value;
                        }
                    }
                }
            }
            // 11 当在同一个节点的数目达到TREEIFY_THRESHOLD(8)个的时候，则扩张数组或将给节点的数据转为树。
            if (binCount != 0) {
                if (binCount >= TREEIFY_THRESHOLD)
                    treeifyBin(tab, i);
                if (oldVal != null)
                    return oldVal;
                break;
            }
        }
    }
    // 12 执行addCount()方法尝试更新元素个数baseCount。
    addCount(1L, binCount);
    return null;
}
```

**11、Java集合的快速失败机制“fail-fast”，以及安全失败“fail-safe”。**

“fail-fast”是java集合的一种错误检测机制，当多个线程对集合进行结构上的改变的操作时，有可能会产生 fail-fast 机制。

例如：假设存在两个线程（线程1、线程2），线程1通过Iterator在遍历集合A中的元素，在某个时候线程2修改了集合A的结构（是结构上面的修改，而不是简单的修改集合元素的内容），那么这个时候程序就会抛出 ConcurrentModificationException 异常，从而产生fail-fast机制。

原因：迭代器在遍历时直接访问集合中的内容，并且在遍历过程中使用一个 modCount 变量。集合在被遍历期间如果内容发生变化，就会改变modCount的值。每当迭代器使用hashNext()/next()遍历下一个元素之前，都会检测modCount变量是否为expectedmodCount值，是的话就返回遍历；否则抛出异常ConcurrentModification，终止遍历。

来看一下ArrayList源码部分，在next方法执行时，会先执行checkForComodification方法来检查。

```java
public E next() {
    checkForComodification();
    int i = cursor;
    if (i >= size)
        throw new NoSuchElementException();
    Object[] elementData = ArrayList.this.elementData;
    if (i >= elementData.length)
        throw new ConcurrentModificationException();
    cursor = i + 1;
    return (E) elementData[lastRet = i];
}
final void checkForComodification() {
    if (modCount != expectedModCount)
        throw new ConcurrentModificationException();
}
```

java.util包下的集合类都是快速失败机制，不能在多线程下发生并修改（迭代过程中被修改）。

解决办法：

- 1在遍历过程中，所有涉及到改变modCount值得地方全部加上synchronized。
- 使用CopyOnWriteArrayList来替换ArrayList。

**与“fail-fast”对应的是“fail-safe”。**

采用安全失败机制的集合容器,在遍历时不是直接在集合内容上访问的,而是先copy原有集合内容,在拷贝的集合上进行遍历。由于迭代时是对原集合的拷贝的值进行遍历,所以在遍历过程中对原集合所作的修改并不能被迭代器检测到,所以不会触发ConcurrentModificationException异常。

基于拷贝内容的迭代虽然避免了ConcurrentModificationException异常，但同样地，迭代器并不能访问到修改后的内容，简单来说，迭代器遍历的是开始遍历那一刻拿到的集合拷贝，在遍历期间原集合发生的修改迭代器行为是不知道的。

java.util.concurrent包下的容器都是安全失败的,可以在多线程下并发使用,并发修改。

**12、ArrayList和CopyOnWriteArrayList的区别。**

ArrayList底层就是数组实现的，查询的时候直接根据索引就可以很快的找到对应的元素，改也是如此，找到index对应元素直接替换。而增加和删除就涉及到数据元素的移动，会比较慢。

需要注意的是，ArrayList扩容时，`newCapacity = oldCapacity*1.5`。看一下grow函数。

```java
private void grow(int minCapacity) {
    // overflow-conscious code
    int oldCapacity = elementData.length;
    int newCapacity = oldCapacity + (oldCapacity >> 1);//1.5倍
    if (newCapacity - minCapacity < 0)
        newCapacity = minCapacity;
    if (newCapacity - MAX_ARRAY_SIZE > 0)
        newCapacity = hugeCapacity(minCapacity);
    // minCapacity is usually close to size, so this is a win:
    elementData = Arrays.copyOf(elementData, newCapacity);
}
```

主要看CopyOnWriteArrayList，主要是读写分离，写入将导致创建整个底层数组的副本，而源数组将保留在原地，使得复制的数组在被修改时，读取操作可以安全地执行，看一下源码。

```java
private transient volatile Object[] array;//保存元素对象的动态数组，注意是用volatile transient声明，只能通过setArray()/getArray()访问。
final transient ReentrantLock lock = new ReentrantLock();//类内唯一一把锁
final Object[] getArray() {
    return array;
}
final void setArray(Object[] a) {
    array = a;
}
//添加操作
public boolean add(E e) {
    final ReentrantLock lock = this.lock;
    lock.lock();//获得锁
    try {
        Object[] elements = getArray();//复制一个新的数组
        int len = elements.length;
        Object[] newElements = Arrays.copyOf(elements, len + 1);
        newElements[len] = e;//插入新值
        setArray(newElements);//将新的数组指向原来的引用
        return true;
    } finally {
        lock.unlock();//释放锁
    }
}
//删除操作
public E remove(int index) {
    final ReentrantLock lock = this.lock;
    lock.lock();
    try {
        Object[] elements = getArray();
        int len = elements.length;
        E oldValue = get(elements, index);
        int numMoved = len - index - 1;
        if (numMoved == 0)//如果删除的是最后一个元素，直接复制出来前面len-1元素即可。
            setArray(Arrays.copyOf(elements, len - 1));
        else {//如果删除的是中间元素，就需要分别复制前后元素了。
            Object[] newElements = new Object[len - 1];
            System.arraycopy(elements, 0, newElements, 0, index);
            System.arraycopy(elements, index + 1, newElements, index,
                             numMoved);
            setArray(newElements);
        }
        return oldValue;
    } finally {
        lock.unlock();
    }
}
//读操作，不需要加锁。
public E get(int index) {
    return get(getArray(), index);
}
private E get(Object[] a, int index) {
    return (E) a[index];
}
```

CopyOnWriteArrayList增删改都需要获得锁，并且只有一把锁，而读操作不需要锁，支持并发。注意， CopyOnWriteArrayList**没有扩容机制**，每次增删操作都是新建数组操作。再解释一下CopyOnWriteArrayList为什么是“fail-safe”，因为在迭代器遍历开始时，获取的是当前数组，而后续的增删改，都是新建数组操作，不会影响到迭代器。

另外，CopyOnWriteArrayList的并发安全性能比vector要好，vector是增删改查方法都加了synchronized关键字，但是每个方法执行的时候都要去获得锁，性能就会大大下降，而CopyOnWriteArrayList只是在增删改上加锁，读不加锁，在读方面的性能好于vector，CopyOnWriteArrayList支持读多写少的并发情况。

但需要注意的点有

- 由于CopyOnWriteArrayList写操作的时候，需要拷贝数组，会消耗内存。如果原数组内容比较多的情况下，可能会导致young gc或者full gc。
- 不能用于实时读写的场景，像拷贝数组、新增元素都需要时间。虽然CopyOnWriteArrayList 能做到最终一致性,但是还是没法满足实时性要求。

**13、ArrayList和LinkedList的区别。**

主要有以下几点区别：

- LinkedList实现了List和Deque接口，一般称为双向链表；ArrayList实现了List接口，是动态数组。
- LinkedList在插入和删除数据时效率更高，ArrayList在查找某个index的数据时效率更高。
- LinkedList比ArrayList需要更多内存。

**14、HashSet是如何保证数据不可重复的。**

HashSet的底层其实就是HashMap，只不过HashSet是实现了Set接口，并且把数据作为Key值，而Value值一直使用一个相同的虚值来保存。

```java
private transient HashMap<E,Object> map;//底层实现的HashMap
private static final Object PRESENT = new Object();//虚值
public HashSet() {
    map = new HashMap<>();
}
public boolean add(E e) {
    return map.put(e, PRESENT)==null;//如果插入重复Value，map会返回上次插入的旧值PRENET，就不为null，这句话就返回false，代表插入失败。
}
```

由于HashMap的K值本身就不允许重复，并且在HashMap中如果K/V相同时，会用新的V覆盖掉旧的V，然后返回旧的V，那么在HashSet中执行这一句话始终会返回一个false，导致插入失败，这样就保证了数据的不可重复性。

**15、BlockingQueue是什么。**

java.util.concurrent.BlockingQueue是一个队列，在进行检索或移除一个元素的时候，它会等待队列变为非空；当添加一个元素时，它会等待队列中的可用空间。

BlockingQueue接口是java集合框架的一部分，主要用于实现生产者-消费者模式。这样我们就不需要担心等待生产者有可用的空间，以及消费者有可用的对象。因为它们都在BlockingQueue的实现类中被处理了。

Java提供了几种BlockingQueue的实现，比如ArrayBlockingQueue、LinkedBlockingQueue、PriorityBlockingQueue,、SynchronousQueue等。

**16、HashMap为什么选用红黑树。**

HashMap在jdk1.8之后引入了红黑树的概念，表示若桶中链表元素超过8时，会自动转化成红黑树；若桶中元素小于等于6时，树结构还原成链表形式。

因为红黑树的平均查找长度是log(n)，长度为8的时候，平均查找长度为3，如果继续使用链表，平均查找长度为8/2=4，这才有转换为树的必要。链表长度如果是小于等于6，6/2=3，虽然速度也很快的，但是转化为树结构和生成树的时间并不会太短。

并且，红黑树不像平衡树那样追求绝对的平衡，省去了很多没有必要的调平衡操作，插入删除元素等操作效率提高了很多。

另外，上下阈值选择6和8的情况下，中间有个差值7可以防止链表和树之间频繁的转换。假设一下，如果设计成链表个数超过8则链表转换成树结构，链表个数小于8则树结构转换成链表，如果一个HashMap不停的插入、删除元素，链表个数在8左右徘徊，就会频繁的发生树转链表、链表转树，效率会很低。

**17、Iterator中是否存在Add方法。**

Iterator是没有add方法的，但是有remove方法。Iterator.remove()是唯一安全的方式来在迭代过程中修改集合；如果在迭代过程中以任何其它的方式修改了基本集合将会产生未知的行为。而且每调用一次next()方法，remove()方法只能被调用一次，如果违反这个规则将抛出一个异常。

ListIterator是一个功能更强大的迭代器，继承于Iterator，可以通过调用listIterator()方法产生一个指向List开始处的ListIterator。它内部实现了一个双向链表，是具有add方法的。

两者的主要区别如下：

- ListIterator有add()方法，可以在遍历的时候向List中添加对象，而Iterator不可以。
- ListIterator和Iterator都有hasNext()和next()方法，可以实现顺序往后遍历，但是ListIterator有hasPrevious()和previous()方法，可以实现逆向遍历，Iterator就不可以。
- ListIterator可以定位当前的索引位置，nextIndex()和previousIndex()可以实现。Iterator没有这个功能。
- 都可以实现删除对象，但是ListIterator可以实现对象的修改，set()方法可以实现。Iterator仅能遍历，不能修改。

弥有，2019年9月
[EOF]
